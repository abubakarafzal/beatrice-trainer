{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abubakarafzal/beatrice-trainer/blob/copy/beatrice_trainer_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7keHA5Kl2aN"
      },
      "source": [
        "# Beatrice Trainer - Google Colab\n",
        "\n",
        "This notebook allows you to train Beatrice voice conversion models on Google Colab with T4 GPU support.\n",
        "\n",
        "## Instructions:\n",
        "1. Make sure to select a GPU runtime: Runtime → Change runtime type → GPU (T4)\n",
        "2. Your training data is already in the repository at `datasets/model_1/`\n",
        "3. Your configuration is already set in `datasets/model_1_config_lowmem.json`\n",
        "4. Run all cells to start training with your existing setup\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LMGbnGibl2aP"
      },
      "source": [
        "## 1. Setup Environment\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "iX8gSARZl2aP",
        "outputId": "f33b549b-5ca6-48c5-ce50-5d4e7f6078da",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/261.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m261.0/261.0 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m47.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pyworld (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "CUDA available: False\n"
          ]
        }
      ],
      "source": [
        "# Install dependencies\n",
        "!pip install -q torch torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "!pip install -q tqdm numpy tensorboard soundfile pyworld ipynbname\n",
        "\n",
        "# Verify GPU\n",
        "import torch\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"CUDA version: {torch.version.cuda}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "BkUoqjfZl2aQ",
        "outputId": "2d74b414-1644-426f-a315-6f41fa2c065a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current directory: /content/beatrice-trainer\n"
          ]
        }
      ],
      "source": [
        "# Clone the repository\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "if not os.path.exists(\"beatrice-trainer\"):\n",
        "    !git clone -q https://github.com/abubakarafzal/beatrice-trainer.git\n",
        "    !cd beatrice-trainer && git lfs pull\n",
        "\n",
        "os.chdir(\"beatrice-trainer\")\n",
        "print(f\"Current directory: {os.getcwd()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-CMkbR3pl2aQ"
      },
      "source": [
        "## 2. Verify Training Data\n",
        "\n",
        "Your training data is already in the repository at `datasets/model_1/`. The structure should be:\n",
        "```\n",
        "datasets/model_1/\n",
        "  - my/\n",
        "    - b1.wav\n",
        "    - ...\n",
        "```\n",
        "\n",
        "**Note**: The data is already configured and ready to use.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "68VKQgBql2aQ"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# Use existing data directory from repository\n",
        "data_dir = Path(\"datasets/model_1\")\n",
        "\n",
        "# Verify data exists\n",
        "if data_dir.exists():\n",
        "    print(f\"✓ Found training data directory: {data_dir}\")\n",
        "    print(\"\\nTraining data structure:\")\n",
        "    for item in data_dir.rglob(\"*.wav\"):\n",
        "        print(f\"  {item.relative_to(data_dir)}\")\n",
        "    for item in data_dir.rglob(\"*.flac\"):\n",
        "        print(f\"  {item.relative_to(data_dir)}\")\n",
        "    print(f\"\\nTotal audio files found: {len(list(data_dir.rglob('*.wav'))) + len(list(data_dir.rglob('*.flac')))}\")\n",
        "else:\n",
        "    raise FileNotFoundError(f\"Training data directory not found: {data_dir}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hbO-u65gl2aR"
      },
      "source": [
        "## 3. Load Training Configuration\n",
        "\n",
        "Your training configuration is already set in `datasets/model_1_config_lowmem.json`. This configuration is optimized for T4 GPU (16GB VRAM).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5tv5Msnll2aR"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "# Load your existing config file\n",
        "config_path = Path(\"datasets/model_1_config_lowmem.json\")\n",
        "\n",
        "if not config_path.exists():\n",
        "    raise FileNotFoundError(f\"Config file not found: {config_path}\")\n",
        "\n",
        "print(f\"✓ Loading configuration from: {config_path}\")\n",
        "\n",
        "# Load your existing config\n",
        "with open(config_path, \"r\") as f:\n",
        "    config = json.load(f)\n",
        "\n",
        "# Add data_dir and out_dir to your config\n",
        "config[\"data_dir\"] = str(data_dir)  # Use your existing data directory\n",
        "config[\"out_dir\"] = \"/content/outputs\"  # Output directory in Colab\n",
        "\n",
        "# Save the updated config for training\n",
        "training_config_path = Path(\"training_config.json\")\n",
        "with open(training_config_path, \"w\") as f:\n",
        "    json.dump(config, f, indent=2)\n",
        "\n",
        "print(\"\\nTraining configuration:\")\n",
        "print(f\"  Data directory: {config['data_dir']}\")\n",
        "print(f\"  Output directory: {config['out_dir']}\")\n",
        "print(f\"  Batch size: {config['batch_size']}\")\n",
        "print(f\"  Hidden channels: {config['hidden_channels']}\")\n",
        "print(f\"  Total steps: {config['n_steps']}\")\n",
        "print(f\"  Wav length: {config['wav_length']}\")\n",
        "print(f\"  Segment length: {config['segment_length']}\")\n",
        "print(f\"  Use AMP: {config['use_amp']}\")\n",
        "print(f\"\\n✓ Configuration saved to: {training_config_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vgjIsvR4l2aR"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3J9vljDzl2aR"
      },
      "outputs": [],
      "source": [
        "# Import and run training\n",
        "import sys\n",
        "import os\n",
        "sys.path.insert(0, '/content/beatrice-trainer')\n",
        "os.chdir('/content/beatrice-trainer')\n",
        "\n",
        "# Modify the notebook detection to work in Colab\n",
        "import beatrice_trainer.__main__ as trainer_module\n",
        "from contextlib import nullcontext\n",
        "from tqdm.auto import tqdm\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchaudio\n",
        "from torch.nn import functional as F\n",
        "import gc\n",
        "import gzip\n",
        "import shutil\n",
        "from collections import defaultdict\n",
        "\n",
        "# Override the prepare_training_configs_for_experiment function for Colab\n",
        "def prepare_training_configs_for_colab():\n",
        "    from copy import deepcopy\n",
        "    from pathlib import Path\n",
        "    import json\n",
        "\n",
        "    # Load config\n",
        "    config_path = Path(\"training_config.json\")\n",
        "    with open(config_path, \"r\") as f:\n",
        "        h = json.load(f)\n",
        "\n",
        "    data_dir = Path(h.pop(\"data_dir\"))\n",
        "    out_dir = Path(h.pop(\"out_dir\"))\n",
        "\n",
        "    # Fill in defaults for any missing keys\n",
        "    default_hparams = trainer_module.dict_default_hparams\n",
        "    for key in default_hparams.keys():\n",
        "        if key not in h:\n",
        "            h[key] = default_hparams[key]\n",
        "\n",
        "    return h, data_dir, out_dir, False, False\n",
        "\n",
        "# Replace the function\n",
        "trainer_module.prepare_training_configs_for_experiment = prepare_training_configs_for_colab\n",
        "\n",
        "# Prepare training (this sets up everything)\n",
        "(\n",
        "    device,\n",
        "    in_wav_dataset_dir,\n",
        "    h,\n",
        "    out_dir,\n",
        "    speakers,\n",
        "    test_filelist,\n",
        "    training_loader,\n",
        "    speaker_f0s,\n",
        "    test_pitch_shifts,\n",
        "    phone_extractor,\n",
        "    pitch_estimator,\n",
        "    net_g,\n",
        "    net_d,\n",
        "    optim_g,\n",
        "    optim_d,\n",
        "    grad_scaler,\n",
        "    grad_balancer,\n",
        "    resample_to_in_sample_rate,\n",
        "    initial_iteration,\n",
        "    scheduler_g,\n",
        "    scheduler_d,\n",
        "    dict_scalars,\n",
        "    quality_tester,\n",
        "    writer,\n",
        ") = trainer_module.prepare_training()\n",
        "\n",
        "# Import necessary functions and classes\n",
        "from beatrice_trainer.__main__ import (\n",
        "    ConvNeXtStack,\n",
        "    MultiPeriodDiscriminator,\n",
        "    PhoneExtractor,\n",
        "    PitchEstimator,\n",
        "    ConverterNetwork,\n",
        "    get_resampler,\n",
        "    compute_grad_norm,\n",
        "    get_compressed_optimizer_state_dict,\n",
        "    PARAPHERNALIA_VERSION,\n",
        "    repo_root,\n",
        ")\n",
        "\n",
        "# Training loop (adapted from __main__ block)\n",
        "if writer is not None:\n",
        "    if h.compile_convnext:\n",
        "        raw_convnextstack_forward = ConvNeXtStack.forward\n",
        "        compiled_convnextstack_forward = torch.compile(\n",
        "            ConvNeXtStack.forward, mode=\"reduce-overhead\"\n",
        "        )\n",
        "    if h.compile_d4c:\n",
        "        d4c = torch.compile(d4c, mode=\"reduce-overhead\")\n",
        "    if h.compile_discriminator:\n",
        "        MultiPeriodDiscriminator.forward_and_compute_loss = torch.compile(\n",
        "            MultiPeriodDiscriminator.forward_and_compute_loss, mode=\"reduce-overhead\"\n",
        "        )\n",
        "\n",
        "    # Training loop\n",
        "    with (\n",
        "        torch.profiler.profile(\n",
        "            schedule=torch.profiler.schedule(wait=1500, warmup=10, active=5, repeat=1),\n",
        "            on_trace_ready=torch.profiler.tensorboard_trace_handler(out_dir),\n",
        "            record_shapes=True,\n",
        "            with_stack=True,\n",
        "            profile_memory=True,\n",
        "            with_flops=True,\n",
        "        )\n",
        "        if h.profile\n",
        "        else nullcontext()\n",
        "    ) as profiler:\n",
        "        data_iter = iter(training_loader)\n",
        "        for iteration in tqdm(range(initial_iteration, h.n_steps), desc=\"Training\"):\n",
        "            # === 1. データ前処理 ===\n",
        "            try:\n",
        "                batch = next(data_iter)\n",
        "            except (NameError, StopIteration):\n",
        "                data_iter = iter(training_loader)\n",
        "                batch = next(data_iter)\n",
        "            (\n",
        "                clean_wavs,\n",
        "                noisy_wavs_16k,\n",
        "                slice_starts,\n",
        "                speaker_ids,\n",
        "                formant_shift_semitone,\n",
        "            ) = map(lambda x: x.to(device, non_blocking=True), batch)\n",
        "\n",
        "            # === 2. 学習 ===\n",
        "            with torch.amp.autocast(\"cuda\", enabled=h.use_amp):\n",
        "                # === 2.1 Generator の順伝播 ===\n",
        "                if h.compile_convnext:\n",
        "                    ConvNeXtStack.forward = compiled_convnextstack_forward\n",
        "                (\n",
        "                    y,\n",
        "                    y_hat,\n",
        "                    y_hat_for_backward,\n",
        "                    loss_loudness,\n",
        "                    loss_mel,\n",
        "                    loss_ap,\n",
        "                    generator_stats,\n",
        "                ) = net_g.forward_and_compute_loss(\n",
        "                    noisy_wavs_16k[:, None, :],\n",
        "                    speaker_ids,\n",
        "                    formant_shift_semitone,\n",
        "                    slice_start_indices=slice_starts,\n",
        "                    slice_segment_length=h.segment_length,\n",
        "                    y_all=clean_wavs[:, None, :],\n",
        "                    enable_loss_ap=h.grad_weight_ap != 0.0,\n",
        "                )\n",
        "                if h.compile_convnext:\n",
        "                    ConvNeXtStack.forward = raw_convnextstack_forward\n",
        "                assert y_hat.isfinite().all()\n",
        "                assert loss_loudness.isfinite().all()\n",
        "                assert loss_mel.isfinite().all()\n",
        "                assert loss_ap.isfinite().all()\n",
        "\n",
        "                # === 2.2 Discriminator の順伝播 ===\n",
        "                loss_discriminator, loss_adv, loss_fm, discriminator_stats = (\n",
        "                    net_d.forward_and_compute_loss(y, y_hat)\n",
        "                )\n",
        "                assert loss_discriminator.isfinite().all()\n",
        "                assert loss_adv.isfinite().all()\n",
        "                assert loss_fm.isfinite().all()\n",
        "\n",
        "            # === 2.3 Discriminator の逆伝播 ===\n",
        "            for param in net_d.parameters():\n",
        "                assert param.grad is None\n",
        "            grad_scaler.scale(loss_discriminator).backward(\n",
        "                retain_graph=True, inputs=list(net_d.parameters())\n",
        "            )\n",
        "            loss_discriminator = loss_discriminator.item()\n",
        "            grad_scaler.unscale_(optim_d)\n",
        "            if iteration % 5 == 0:\n",
        "                grad_norm_d, d_grad_norm_stats = compute_grad_norm(net_d, True)\n",
        "            else:\n",
        "                grad_norm_d = math.nan\n",
        "                d_grad_norm_stats = {}\n",
        "\n",
        "            # === 2.4 Generator の逆伝播 ===\n",
        "            for param in net_g.parameters():\n",
        "                assert param.grad is None\n",
        "            gradient_balancer_stats = grad_balancer.backward(\n",
        "                {\n",
        "                    \"loss_loudness\": loss_loudness,\n",
        "                    \"loss_mel\": loss_mel,\n",
        "                    \"loss_adv\": loss_adv,\n",
        "                    \"loss_fm\": loss_fm,\n",
        "                }\n",
        "                | ({\"loss_ap\": loss_ap} if h.grad_weight_ap else {}),\n",
        "                y_hat_for_backward,\n",
        "                grad_scaler,\n",
        "                skip_update_ema=iteration > 10 and iteration % 5 != 0,\n",
        "            )\n",
        "            loss_loudness = loss_loudness.item()\n",
        "            loss_mel = loss_mel.item()\n",
        "            loss_adv = loss_adv.item()\n",
        "            loss_fm = loss_fm.item()\n",
        "            if h.grad_weight_ap:\n",
        "                loss_ap = loss_ap.item()\n",
        "            grad_scaler.unscale_(optim_g)\n",
        "            if iteration % 5 == 0:\n",
        "                grad_norm_g, g_grad_norm_stats = compute_grad_norm(net_g, True)\n",
        "            else:\n",
        "                grad_norm_g = math.nan\n",
        "                g_grad_norm_stats = {}\n",
        "\n",
        "            # === 2.5 パラメータの更新 ===\n",
        "            grad_scaler.step(optim_g)\n",
        "            optim_g.zero_grad(set_to_none=True)\n",
        "            grad_scaler.step(optim_d)\n",
        "            optim_d.zero_grad(set_to_none=True)\n",
        "            grad_scaler.update()\n",
        "\n",
        "            # === 3. ログ ===\n",
        "            dict_scalars[\"loss_g/loss_loudness\"].append(loss_loudness)\n",
        "            dict_scalars[\"loss_g/loss_mel\"].append(loss_mel)\n",
        "            if h.grad_weight_ap:\n",
        "                dict_scalars[\"loss_g/loss_ap\"].append(loss_ap)\n",
        "            dict_scalars[\"loss_g/loss_fm\"].append(loss_fm)\n",
        "            dict_scalars[\"loss_g/loss_adv\"].append(loss_adv)\n",
        "            dict_scalars[\"other/grad_scale\"].append(grad_scaler.get_scale())\n",
        "            dict_scalars[\"loss_d/loss_discriminator\"].append(loss_discriminator)\n",
        "            if math.isfinite(grad_norm_d):\n",
        "                dict_scalars[\"other/gradient_norm_d\"].append(grad_norm_d)\n",
        "                for name, value in d_grad_norm_stats.items():\n",
        "                    dict_scalars[f\"~gradient_norm_d/{name}\"].append(value)\n",
        "            if math.isfinite(grad_norm_g):\n",
        "                dict_scalars[\"other/gradient_norm_g\"].append(grad_norm_g)\n",
        "                for name, value in g_grad_norm_stats.items():\n",
        "                    dict_scalars[f\"~gradient_norm_g/{name}\"].append(value)\n",
        "            dict_scalars[\"other/lr_g\"].append(scheduler_g.get_last_lr()[0])\n",
        "            dict_scalars[\"other/lr_d\"].append(scheduler_d.get_last_lr()[0])\n",
        "            for k, v in generator_stats.items():\n",
        "                dict_scalars[f\"~loss_generator/{k}\"].append(v)\n",
        "            for k, v in discriminator_stats.items():\n",
        "                dict_scalars[f\"~loss_discriminator/{k}\"].append(v)\n",
        "            for k, v in gradient_balancer_stats.items():\n",
        "                dict_scalars[f\"~gradient_balancer/{k}\"].append(v)\n",
        "\n",
        "            if (iteration + 1) % 1000 == 0 or iteration == 0:\n",
        "                for name, scalars in dict_scalars.items():\n",
        "                    if scalars:\n",
        "                        writer.add_scalar(\n",
        "                            name, sum(scalars) / len(scalars), iteration + 1\n",
        "                        )\n",
        "                        scalars.clear()\n",
        "\n",
        "            # === 4. 検証 ===\n",
        "            if (iteration + 1) % h.evaluation_interval == 0 or iteration + 1 in {\n",
        "                1,\n",
        "                h.n_steps,\n",
        "            }:\n",
        "                torch.backends.cudnn.benchmark = False\n",
        "                net_g.eval()\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "                dict_qualities_all = defaultdict(list)\n",
        "                n_added_wavs = 0\n",
        "                with torch.inference_mode():\n",
        "                    for i, ((file, target_ids), pitch_shift_semitones) in enumerate(\n",
        "                        zip(test_filelist, test_pitch_shifts)\n",
        "                    ):\n",
        "                        source_wav, sr = torchaudio.load(file, backend=\"soundfile\")\n",
        "                        source_wav = source_wav.to(device)\n",
        "                        if sr != h.in_sample_rate:\n",
        "                            source_wav = get_resampler(sr, h.in_sample_rate, device)(\n",
        "                                source_wav\n",
        "                            )\n",
        "                        source_wav = source_wav.to(device)\n",
        "                        original_source_wav_length = source_wav.size(1)\n",
        "                        if source_wav.size(1) % h.in_sample_rate == 0:\n",
        "                            padded_source_wav = source_wav\n",
        "                        else:\n",
        "                            padded_source_wav = F.pad(\n",
        "                                source_wav,\n",
        "                                (\n",
        "                                    0,\n",
        "                                    h.in_sample_rate\n",
        "                                    - source_wav.size(1) % h.in_sample_rate,\n",
        "                                ),\n",
        "                            )\n",
        "                        converted = net_g(\n",
        "                            padded_source_wav[[0] * len(target_ids), None],\n",
        "                            torch.tensor(target_ids, device=device),\n",
        "                            torch.tensor(\n",
        "                                [0.0] * len(target_ids), device=device\n",
        "                            ),\n",
        "                            torch.tensor(\n",
        "                                [float(p) for p in pitch_shift_semitones], device=device\n",
        "                            ),\n",
        "                        ).squeeze_(1)[:, : original_source_wav_length // 160 * 240]\n",
        "                        if i < 12:\n",
        "                            if iteration == 0:\n",
        "                                writer.add_audio(\n",
        "                                    f\"source/y_{i:02d}\",\n",
        "                                    source_wav,\n",
        "                                    iteration + 1,\n",
        "                                    h.in_sample_rate,\n",
        "                                )\n",
        "                            for d in range(\n",
        "                                min(\n",
        "                                    len(target_ids),\n",
        "                                    1 + (12 - i - 1) // len(test_filelist),\n",
        "                                )\n",
        "                            ):\n",
        "                                idx_in_batch = n_added_wavs % len(target_ids)\n",
        "                                writer.add_audio(\n",
        "                                    f\"converted/y_hat_{i:02d}_{target_ids[idx_in_batch]:03d}_{pitch_shift_semitones[idx_in_batch]:+02d}\",\n",
        "                                    converted[idx_in_batch],\n",
        "                                    iteration + 1,\n",
        "                                    h.out_sample_rate,\n",
        "                                )\n",
        "                                n_added_wavs += 1\n",
        "                        converted = resample_to_in_sample_rate(converted)\n",
        "                        quality = quality_tester.test(converted, source_wav)\n",
        "                        for metric_name, values in quality.items():\n",
        "                            dict_qualities_all[metric_name].extend(values)\n",
        "                dict_qualities = {\n",
        "                    metric_name: sum(values) / len(values)\n",
        "                    for metric_name, values in dict_qualities_all.items()\n",
        "                    if len(values)\n",
        "                }\n",
        "                for metric_name, value in dict_qualities.items():\n",
        "                    writer.add_scalar(f\"validation/{metric_name}\", value, iteration + 1)\n",
        "\n",
        "                net_g.train()\n",
        "                torch.backends.cudnn.benchmark = True\n",
        "                gc.collect()\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "            # === 5. 保存 ===\n",
        "            if (iteration + 1) % h.save_interval == 0 or iteration + 1 in {\n",
        "                1,\n",
        "                h.n_steps,\n",
        "            }:\n",
        "                # チェックポイント\n",
        "                name = f\"{in_wav_dataset_dir.name}_{iteration + 1:08d}\"\n",
        "                checkpoint_file_save = out_dir / f\"checkpoint_{name}.pt.gz\"\n",
        "                if checkpoint_file_save.exists():\n",
        "                    checkpoint_file_save = checkpoint_file_save.with_name(\n",
        "                        f\"{checkpoint_file_save.name}_{hash(None):x}\"\n",
        "                    )\n",
        "                with gzip.open(checkpoint_file_save, \"wb\") as f:\n",
        "                    torch.save(\n",
        "                        {\n",
        "                            \"iteration\": iteration + 1,\n",
        "                            \"net_g\": net_g.state_dict(),\n",
        "                            \"phone_extractor\": phone_extractor.state_dict(),\n",
        "                            \"pitch_estimator\": pitch_estimator.state_dict(),\n",
        "                            \"net_d\": {\n",
        "                                k: v.half() for k, v in net_d.state_dict().items()\n",
        "                            },\n",
        "                            \"optim_g\": get_compressed_optimizer_state_dict(optim_g),\n",
        "                            \"optim_d\": get_compressed_optimizer_state_dict(optim_d),\n",
        "                            \"grad_balancer\": grad_balancer.state_dict(),\n",
        "                            \"grad_scaler\": grad_scaler.state_dict(),\n",
        "                            \"h\": dict(h),\n",
        "                        },\n",
        "                        f,\n",
        "                    )\n",
        "                shutil.copy(checkpoint_file_save, out_dir / \"checkpoint_latest.pt.gz\")\n",
        "\n",
        "                # 推論用\n",
        "                paraphernalia_dir = out_dir / f\"paraphernalia_{name}\"\n",
        "                if paraphernalia_dir.exists():\n",
        "                    paraphernalia_dir = paraphernalia_dir.with_name(\n",
        "                        f\"{paraphernalia_dir.name}_{hash(None):x}\"\n",
        "                    )\n",
        "                paraphernalia_dir.mkdir()\n",
        "                phone_extractor_fp16 = PhoneExtractor()\n",
        "                phone_extractor_fp16.load_state_dict(phone_extractor.state_dict())\n",
        "                phone_extractor_fp16.remove_weight_norm()\n",
        "                phone_extractor_fp16.merge_weights()\n",
        "                phone_extractor_fp16.half()\n",
        "                phone_extractor_fp16.dump(paraphernalia_dir / \"phone_extractor.bin\")\n",
        "                del phone_extractor_fp16\n",
        "                pitch_estimator_fp16 = PitchEstimator()\n",
        "                pitch_estimator_fp16.load_state_dict(pitch_estimator.state_dict())\n",
        "                pitch_estimator_fp16.merge_weights()\n",
        "                pitch_estimator_fp16.half()\n",
        "                pitch_estimator_fp16.dump(paraphernalia_dir / \"pitch_estimator.bin\")\n",
        "                del pitch_estimator_fp16\n",
        "                net_g_fp16 = ConverterNetwork(\n",
        "                    nn.Module(),\n",
        "                    nn.Module(),\n",
        "                    len(speakers),\n",
        "                    h.pitch_bins,\n",
        "                    h.hidden_channels,\n",
        "                    h.vq_topk,\n",
        "                    h.training_time_vq,\n",
        "                    h.phone_noise_ratio,\n",
        "                    h.floor_noise_level,\n",
        "                )\n",
        "                net_g_fp16.load_state_dict(net_g.state_dict())\n",
        "                net_g_fp16.merge_weights()\n",
        "                net_g_fp16.half()\n",
        "                net_g_fp16.dump(paraphernalia_dir / \"waveform_generator.bin\")\n",
        "                net_g_fp16.dump_speaker_embeddings(\n",
        "                    paraphernalia_dir / \"speaker_embeddings.bin\"\n",
        "                )\n",
        "                net_g_fp16.dump_embedding_setter(\n",
        "                    paraphernalia_dir / \"embedding_setter.bin\"\n",
        "                )\n",
        "                del net_g_fp16\n",
        "                shutil.copy(\n",
        "                    repo_root() / \"assets/images/noimage.png\", paraphernalia_dir\n",
        "                )\n",
        "                with open(\n",
        "                    paraphernalia_dir / f\"beatrice_paraphernalia_{name}.toml\",\n",
        "                    \"w\",\n",
        "                    encoding=\"utf-8\",\n",
        "                ) as f:\n",
        "                    f.write(\n",
        "                        f'''[model]\n",
        "version = \"{PARAPHERNALIA_VERSION}\"\n",
        "name = \"{name}\"\n",
        "description = \"\"\"\n",
        "No description for this model.\n",
        "このモデルの説明はありません。\n",
        "\"\"\"\n",
        "'''\n",
        "                    )\n",
        "                    for speaker_id, (speaker, speaker_f0) in enumerate(\n",
        "                        zip(speakers, speaker_f0s)\n",
        "                    ):\n",
        "                        average_pitch = 69.0 + 12.0 * math.log2(speaker_f0 / 440.0)\n",
        "                        average_pitch = round(average_pitch * 8.0) / 8.0\n",
        "                        f.write(\n",
        "                            f'''\n",
        "[voice.{speaker_id}]\n",
        "name = \"{speaker}\"\n",
        "description = \"\"\"\n",
        "No description for this voice.\n",
        "この声の説明はありません。\n",
        "\"\"\"\n",
        "average_pitch = {average_pitch}\n",
        "\n",
        "[voice.{speaker_id}.portrait]\n",
        "path = \"noimage.png\"\n",
        "description = \"\"\"\n",
        "\"\"\"\n",
        "'''\n",
        "                        )\n",
        "\n",
        "            # === 6. スケジューラ更新 ===\n",
        "            scheduler_g.step()\n",
        "            scheduler_d.step()\n",
        "            if h.profile:\n",
        "                profiler.step()\n",
        "\n",
        "    print(\"Training finished.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z9A7sYC-l2aS"
      },
      "source": [
        "## 5. Download Results\n",
        "\n",
        "After training completes, download your trained model from the output directory.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YmAufnAAl2aS"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import zipfile\n",
        "from google.colab import files\n",
        "\n",
        "output_dir = Path(\"/content/outputs\")\n",
        "\n",
        "# Find the latest paraphernalia directory\n",
        "paraphernalia_dirs = list(output_dir.glob(\"paraphernalia_*\"))\n",
        "if paraphernalia_dirs:\n",
        "    latest_dir = max(paraphernalia_dirs, key=lambda p: p.stat().st_mtime)\n",
        "    print(f\"Found model: {latest_dir.name}\")\n",
        "\n",
        "    # Create zip file\n",
        "    zip_path = Path(f\"/content/{latest_dir.name}.zip\")\n",
        "    with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "        for file in latest_dir.rglob(\"*\"):\n",
        "            if file.is_file():\n",
        "                zipf.write(file, file.relative_to(latest_dir.parent))\n",
        "\n",
        "    print(f\"\\nDownloading {zip_path.name}...\")\n",
        "    files.download(str(zip_path))\n",
        "    print(\"\\nDownload complete!\")\n",
        "else:\n",
        "    print(\"No paraphernalia directory found. Training may not have completed yet.\")\n",
        "    print(\"Available files in output directory:\")\n",
        "    for item in output_dir.iterdir():\n",
        "        print(f\"  {item.name}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EL1mPXKEl2aS"
      },
      "source": [
        "## 6. Resume Training (Optional)\n",
        "\n",
        "If training was interrupted, you can resume from the latest checkpoint.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LTV6J2Otl2aS"
      },
      "outputs": [],
      "source": [
        "# Modify the resume function for Colab\n",
        "def prepare_training_configs_for_colab_resume():\n",
        "    from copy import deepcopy\n",
        "    from pathlib import Path\n",
        "    import json\n",
        "\n",
        "    # Load config\n",
        "    config_path = Path(\"training_config.json\")\n",
        "    with open(config_path, \"r\") as f:\n",
        "        h = json.load(f)\n",
        "\n",
        "    data_dir = Path(h.pop(\"data_dir\"))\n",
        "    out_dir = Path(h.pop(\"out_dir\"))\n",
        "\n",
        "    # Fill in defaults for any missing keys\n",
        "    default_hparams = trainer_module.dict_default_hparams\n",
        "    for key in default_hparams.keys():\n",
        "        if key not in h:\n",
        "            h[key] = default_hparams[key]\n",
        "\n",
        "    return h, data_dir, out_dir, True, False  # Set resume=True\n",
        "\n",
        "# Replace the function\n",
        "trainer_module.prepare_training_configs_for_experiment = prepare_training_configs_for_colab_resume\n",
        "\n",
        "# Run training with resume\n",
        "trainer_module.prepare_training()\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}